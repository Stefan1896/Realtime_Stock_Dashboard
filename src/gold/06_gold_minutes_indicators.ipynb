{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","from delta.tables import DeltaTable\n","\n","# Load Silver table\n","df = spark.table(\"silver_stock_minutes\")\n","\n","# Define windows\n","w3 = Window.partitionBy(\"ticker\").orderBy(\"timestamp\").rowsBetween(-2, 0)\n","w8 = Window.partitionBy(\"ticker\").orderBy(\"timestamp\").rowsBetween(-7, 0)\n","w21 = Window.partitionBy(\"ticker\").orderBy(\"timestamp\").rowsBetween(-20, 0)\n","\n","w5 = Window.partitionBy(\"ticker\").orderBy(\"timestamp\").rowsBetween(-4, 0)\n","w15 = Window.partitionBy(\"ticker\").orderBy(\"timestamp\").rowsBetween(-14, 0)\n","w30 = Window.partitionBy(\"ticker\").orderBy(\"timestamp\").rowsBetween(-29, 0)\n","\n","# Returns\n","df = df.withColumn(\"return_1m\", F.col(\"close\") / F.lag(\"close\").over(Window.partitionBy(\"ticker\").orderBy(\"timestamp\")) - 1)\n","df = df.withColumn(\"return_log_1m\", F.log(F.col(\"close\") / F.lag(\"close\").over(Window.partitionBy(\"ticker\").orderBy(\"timestamp\"))))\n","\n","# EMAs (using simple rolling averages as placeholder)\n","df = df.withColumn(\"ema_3\", F.avg(\"close\").over(w3))\n","df = df.withColumn(\"ema_8\", F.avg(\"close\").over(w8))\n","df = df.withColumn(\"ema_21\", F.avg(\"close\").over(w21))\n","\n","# EMA Cross Signal\n","df = df.withColumn(\"ema_cross_signal\",\n","                   F.when(F.col(\"ema_3\") > F.col(\"ema_8\"), F.lit(\"bullish\"))\n","                    .when(F.col(\"ema_3\") < F.col(\"ema_8\"), F.lit(\"bearish\"))\n","                    .otherwise(F.lit(\"neutral\"))\n",")\n","\n","# Rolling volatility\n","df = df.withColumn(\"volatility_5m\", F.stddev(\"return_1m\").over(w5))\n","df = df.withColumn(\"volatility_15m\", F.stddev(\"return_1m\").over(w15))\n","df = df.withColumn(\"volatility_30m\", F.stddev(\"return_1m\").over(w30))\n","\n","# Rolling high/low\n","df = df.withColumn(\"high_15m\", F.max(\"high\").over(w15))\n","df = df.withColumn(\"low_15m\", F.min(\"low\").over(w15))\n","\n","# Rolling VWAP (30m)\n","df = df.withColumn(\"vwap_30m\",\n","                   F.sum(F.col(\"close\") * F.col(\"volume\")).over(w30) /\n","                   F.sum(\"volume\").over(w30)\n",")\n","\n","# Add ingestion timestamp\n","df = df.withColumn(\"ingestion_time\", F.current_timestamp())\n","\n","# Add date\n","df = df.withColumn(\"date\", F.to_date(\"timestamp\"))\n","\n","# Final column order\n","final_cols = [\n","    \"ticker\", \"timestamp\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n","    \"return_1m\", \"return_log_1m\",\n","    \"ema_3\", \"ema_8\", \"ema_21\",\n","    \"ema_cross_signal\",\n","    \"volatility_5m\", \"volatility_15m\", \"volatility_30m\",\n","    \"high_15m\", \"low_15m\",\n","    \"vwap_30m\",\n","    \"ingestion_time\"\n","]\n","\n","df = df.select(*final_cols)\n","\n","# Save or merge into Gold table\n","gold_table = \"gold_stock_minutes_indicators\"\n","\n","if spark.catalog.tableExists(gold_table):\n","    delta_table = DeltaTable.forName(spark, gold_table)\n","    (\n","        delta_table.alias(\"target\")\n","        .merge(\n","            df.alias(\"source\"),\n","            \"target.ticker = source.ticker AND target.timestamp = source.timestamp\"\n","        )\n","        .whenMatchedUpdateAll()\n","        .whenNotMatchedInsertAll()\n","        .execute()\n","    )\n","else:\n","    df.write.format(\"delta\").saveAsTable(gold_table)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"e05a1651-d6cf-4390-96f5-8e38a75ba200","normalized_state":"finished","queued_time":"2026-01-11T21:57:54.7546606Z","session_start_time":null,"execution_start_time":"2026-01-11T21:57:54.7557879Z","execution_finish_time":"2026-01-11T21:58:08.4011824Z","parent_msg_id":"56061119-c7ad-4a8b-804e-15fa955863b5"},"text/plain":"StatementMeta(, e05a1651-d6cf-4390-96f5-8e38a75ba200, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35c0ebcb-9d15-4667-8267-7b2c76b4e469"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c376541a-203d-4803-8f9a-a569b1645c00"}],"default_lakehouse":"c376541a-203d-4803-8f9a-a569b1645c00","default_lakehouse_name":"stock_project_lakehouse","default_lakehouse_workspace_id":"1dc9d7e2-e8b4-44ea-abc5-3b0e24119da9"}}},"nbformat":4,"nbformat_minor":5}