{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","# 1. Check if dim_date exists\n","tables = [t.name for t in spark.catalog.listTables()]\n","table_exists = \"dim_date\" in tables\n","\n","if table_exists:\n","    existing = spark.table(\"dim_date\")\n","    max_date = existing.agg(F.max(\"Date\")).first()[0]\n","    start_date = F.date_add(F.lit(max_date), 1)\n","else:\n","    existing = None\n","    start_date = F.to_date(F.lit(\"2010-01-01\"))\n","\n","end_date = F.current_date()\n","\n","# 2. Generate candidate dates\n","df = (\n","    spark.range(0, 1)\n","    .select(F.sequence(start_date, end_date).alias(\"Date\"))\n","    .withColumn(\"Date\", F.explode(\"Date\"))\n",")\n","\n","# 2b. Safety-Net: niemals ein Date doppelt schreiben\n","if table_exists:\n","    existing_dates = existing.select(\"Date\")\n","    df = df.join(existing_dates, on=\"Date\", how=\"left_anti\")\n","\n","if df.rdd.isEmpty():\n","    print(\"No new dates to add.\")\n","else:\n","    df = (\n","        df\n","        .withColumn(\"Year\", F.year(\"Date\"))\n","        .withColumn(\"Month\", F.month(\"Date\"))\n","        .withColumn(\"MonthName\", F.date_format(\"Date\", \"MMMM\"))\n","        .withColumn(\"MonthShort\", F.date_format(\"Date\", \"MMM\"))\n","        .withColumn(\"Quarter\", F.concat(F.lit(\"Q\"), F.quarter(\"Date\")))\n","        .withColumn(\"Weekday\", F.dayofweek(\"Date\"))\n","        .withColumn(\"WeekdayName\", F.date_format(\"Date\", \"EEEE\"))\n","        .withColumn(\"IsWeekend\", F.dayofweek(\"Date\").isin(1, 7))\n","    )\n","\n","    df.write.format(\"delta\").mode(\"append\" if table_exists else \"overwrite\").saveAsTable(\"dim_date\")\n","    print(\"dim_date updated.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"779912b0-bc25-4b1c-a59e-b1921fb71b0b","normalized_state":"finished","queued_time":"2026-01-18T16:31:57.5834835Z","session_start_time":null,"execution_start_time":"2026-01-18T16:31:57.5844803Z","execution_finish_time":"2026-01-18T16:32:07.0455826Z","parent_msg_id":"726272c1-5cac-498b-ac26-6ba16efc0bf3"},"text/plain":"StatementMeta(, 779912b0-bc25-4b1c-a59e-b1921fb71b0b, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_date updated.\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"67afea4e-d0ea-4f13-a275-9f8f972ed22a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c376541a-203d-4803-8f9a-a569b1645c00"}],"default_lakehouse":"c376541a-203d-4803-8f9a-a569b1645c00","default_lakehouse_name":"stock_project_lakehouse","default_lakehouse_workspace_id":"1dc9d7e2-e8b4-44ea-abc5-3b0e24119da9"}}},"nbformat":4,"nbformat_minor":5}