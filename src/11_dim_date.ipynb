{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","# Check if dim_date exists\n","tables = [t.name for t in spark.catalog.listTables()]\n","table_exists = \"dim_date\" in tables\n","\n","# Today (UTC-safe)\n","today = F.current_date()\n","\n","# Load existing table if it exist\n","if table_exists:\n","    existing = spark.table(\"dim_date\")\n","\n","    # Remove future dates if they exist\n","    existing = existing.filter(F.col(\"Date\") <= today)\n","\n","    # If table is empty after cleanup → treat as new\n","    if existing.count() == 0:\n","        table_exists = False\n","    else:\n","        max_date = existing.agg(F.max(\"Date\")).first()[0]\n","else:\n","    existing = None\n","\n","\n","# 3. Determine start_date\n","if table_exists:\n","    # Next day after max_date\n","    start_date = F.date_add(F.lit(max_date), 1)\n","else:\n","    # Fresh environment → start from 2010\n","    start_date = F.to_date(F.lit(\"2010-01-01\"))\n","\n","# Clamp start_date to today (never generate future dates)\n","start_date = F.least(start_date, today)\n","\n","# Generate candidate dates\n","df = (\n","    spark.range(0, 1)\n","    .select(F.sequence(start_date, today).alias(\"Date\"))\n","    .withColumn(\"Date\", F.explode(\"Date\"))\n",")\n","\n","# Remove duplicates (anti-join)\n","if table_exists:\n","    existing_dates = existing.select(\"Date\")\n","    df = df.join(existing_dates, on=\"Date\", how=\"left_anti\")\n","\n","\n","# Stop if nothing to add\n","if df.rdd.isEmpty():\n","    print(\"No new dates to add.\")\n","else:\n","\n","    # Add attributes\n","    df = (\n","        df\n","        .withColumn(\"Year\", F.year(\"Date\"))\n","        .withColumn(\"Month\", F.month(\"Date\"))\n","        .withColumn(\"MonthName\", F.date_format(\"Date\", \"MMMM\"))\n","        .withColumn(\"MonthShort\", F.date_format(\"Date\", \"MMM\"))\n","        .withColumn(\"Quarter\", F.concat(F.lit(\"Q\"), F.quarter(\"Date\")))\n","        .withColumn(\"Weekday\", F.dayofweek(\"Date\"))\n","        .withColumn(\"WeekdayName\", F.date_format(\"Date\", \"EEEE\"))\n","        .withColumn(\"IsWeekend\", F.dayofweek(\"Date\").isin(1, 7))\n","    )\n","\n","    # Write to Delta\n","    write_mode = \"append\" if table_exists else \"overwrite\"\n","\n","    df.write.format(\"delta\").mode(write_mode).saveAsTable(\"dim_date\")\n","\n","    print(\"dim_date updated.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"8fb086a9-aab7-428e-b8fd-10c15cf7314e","normalized_state":"finished","queued_time":"2026-01-18T19:50:56.4902891Z","session_start_time":null,"execution_start_time":"2026-01-18T19:50:56.4915296Z","execution_finish_time":"2026-01-18T19:51:20.9317217Z","parent_msg_id":"4eb24b03-5593-4293-b51d-39ba74d4f86e"},"text/plain":"StatementMeta(, 8fb086a9-aab7-428e-b8fd-10c15cf7314e, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_date updated.\n"]}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"67afea4e-d0ea-4f13-a275-9f8f972ed22a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c376541a-203d-4803-8f9a-a569b1645c00"}],"default_lakehouse":"c376541a-203d-4803-8f9a-a569b1645c00","default_lakehouse_name":"stock_project_lakehouse","default_lakehouse_workspace_id":"1dc9d7e2-e8b4-44ea-abc5-3b0e24119da9"}}},"nbformat":4,"nbformat_minor":5}