{"cells":[{"cell_type":"code","source":["# Create default deployment stage parameter\n","deployment_stage_parameters = \"\" "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"ad7d4642-6efa-4dc5-8e20-b7070f7e6107","normalized_state":"finished","queued_time":"2026-01-15T20:14:25.6996146Z","session_start_time":"2026-01-15T20:14:25.7005125Z","execution_start_time":"2026-01-15T20:14:49.9375169Z","execution_finish_time":"2026-01-15T20:14:50.286903Z","parent_msg_id":"05143045-efbf-47f7-b978-ade31537a3ca"},"text/plain":"StatementMeta(, ad7d4642-6efa-4dc5-8e20-b7070f7e6107, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"65589062-2c86-44e3-9f4e-7ba97810a7d4"},{"cell_type":"code","source":["# Use json file saved in DEV lakehouse as workaround to get deployment stage as parameter\n","\n","import json\n","\n","config_dict = json.loads(deployment_stage_parameters)\n","\n","env = config_dict.get(\"environment\", {})\n","default_params = {\n","    # Extract individual values safely\n","    \"stage\": env.get(\"Stage\", \"\")\n","}\n","stage = default_params[\"stage\"]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2026-01-15T20:14:04.1278892Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2026-01-15T20:14:23.5996242Z","parent_msg_id":"07e8d961-a0a9-472c-b6af-5c3cc95f46f9"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"741b6b65-e1f6-4044-b2f5-94cb70bd4041"},{"cell_type":"code","source":["import yfinance as yf \n","import pandas as pd \n","from pyspark.sql.functions import col\n","\n","# Full list of tickers to load in test/prod\n","all_tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\"]\n","\n","# Stage-specific configuration for ticker selection and date range\n","if stage == \"DEV\":\n","    # Small subset for fast development\n","    tickers = [\"AAPL\", \"MSFT\"]\n","    start_date = \"2023-01-01\"   # only 1 year of data\n","elif stage == \"test\":\n","    # Representative dataset for validation\n","    tickers = all_tickers\n","    start_date = \"2021-01-01\"   # last 3 years\n","else:  # prod\n","    # Full dataset for production workloads\n","    tickers = all_tickers\n","    start_date = \"2011-01-01\"   # full history\n","\n","# End date is always yesterday (market data is delayed)\n","end_date = pd.Timestamp.today() - pd.Timedelta(days=1)\n","\n","rows = []\n","\n","for ticker in tickers:\n","\n","    # Download daily historical data for the selected ticker\n","    df_t = yf.download(\n","        ticker,\n","        interval=\"1d\",\n","        start=start_date,\n","        end=end_date,\n","        auto_adjust=True,\n","        group_by=\"column\"\n","    )\n","\n","    # Skip tickers with no available data\n","    if df_t.empty:\n","        print(f\"⚠️ No data for {ticker}, skipping.\")\n","        continue\n","\n","    # Reset index so \"Date\" becomes a column\n","    df_t = df_t.reset_index()\n","\n","    # Remove multi-index columns if present\n","    if isinstance(df_t.columns, pd.MultiIndex):\n","        df_t.columns = [c[0] for c in df_t.columns]\n","\n","    # Rename date column for consistency\n","    df_t = df_t.rename(columns={\"Date\": \"Datetime\"})\n","\n","    # Add ticker column for identification\n","    df_t[\"ticker\"] = ticker\n","\n","    rows.append(df_t)\n","\n","# Combine all tickers into a single flat DataFrame\n","df_flat = pd.concat(rows, ignore_index=True)\n","\n","print(\"Columns in df_flat:\", df_flat.columns)\n","df_flat.sort_values(by=\"Datetime\", ascending=False).head()\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"ad7d4642-6efa-4dc5-8e20-b7070f7e6107","normalized_state":"finished","queued_time":"2026-01-15T20:15:14.1245636Z","session_start_time":null,"execution_start_time":"2026-01-15T20:15:14.12583Z","execution_finish_time":"2026-01-15T20:15:18.536957Z","parent_msg_id":"ea0f318e-a669-43d9-b0a9-d726a1237ba4"},"text/plain":"StatementMeta(, ad7d4642-6efa-4dc5-8e20-b7070f7e6107, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n"]},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"        Datetime       Close        High         Low        Open    Volume  \\\n18904 2026-01-14  439.200012  443.910004  434.220001  442.809998  57259500   \n3780  2026-01-14  259.959991  261.820007  256.709991  259.489990  40019400   \n11342 2026-01-14  335.839996  336.519989  330.480011  335.059998  28525600   \n15123 2026-01-14  236.649994  241.279999  236.220001  241.149994  41410600   \n7561  2026-01-14  459.380005  468.200012  457.170013  466.459991  28184300   \n\n      ticker  \n18904   TSLA  \n3780    AAPL  \n11342  GOOGL  \n15123   AMZN  \n7561    MSFT  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Datetime</th>\n      <th>Close</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Open</th>\n      <th>Volume</th>\n      <th>ticker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18904</th>\n      <td>2026-01-14</td>\n      <td>439.200012</td>\n      <td>443.910004</td>\n      <td>434.220001</td>\n      <td>442.809998</td>\n      <td>57259500</td>\n      <td>TSLA</td>\n    </tr>\n    <tr>\n      <th>3780</th>\n      <td>2026-01-14</td>\n      <td>259.959991</td>\n      <td>261.820007</td>\n      <td>256.709991</td>\n      <td>259.489990</td>\n      <td>40019400</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>11342</th>\n      <td>2026-01-14</td>\n      <td>335.839996</td>\n      <td>336.519989</td>\n      <td>330.480011</td>\n      <td>335.059998</td>\n      <td>28525600</td>\n      <td>GOOGL</td>\n    </tr>\n    <tr>\n      <th>15123</th>\n      <td>2026-01-14</td>\n      <td>236.649994</td>\n      <td>241.279999</td>\n      <td>236.220001</td>\n      <td>241.149994</td>\n      <td>41410600</td>\n      <td>AMZN</td>\n    </tr>\n    <tr>\n      <th>7561</th>\n      <td>2026-01-14</td>\n      <td>459.380005</td>\n      <td>468.200012</td>\n      <td>457.170013</td>\n      <td>466.459991</td>\n      <td>28184300</td>\n      <td>MSFT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c827e87e-ced5-4361-a33a-e126d4057f80"},{"cell_type":"code","source":["df_spark = spark.createDataFrame(df_flat)\n","df_spark = df_spark.withColumn(\"Datetime\", col(\"Datetime\").cast(\"timestamp\"))\n","\n","spark.sql(\"DROP TABLE IF EXISTS bronze_stock_daily\")\n","df_spark.write.format(\"delta\").saveAsTable(\"bronze_stock_daily\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"ad7d4642-6efa-4dc5-8e20-b7070f7e6107","normalized_state":"finished","queued_time":"2026-01-15T20:15:27.8070676Z","session_start_time":null,"execution_start_time":"2026-01-15T20:15:27.8081118Z","execution_finish_time":"2026-01-15T20:15:46.0816664Z","parent_msg_id":"5ec08063-62e6-4364-a0f3-86499e721434"},"text/plain":"StatementMeta(, ad7d4642-6efa-4dc5-8e20-b7070f7e6107, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"199ee3b0-5bff-4457-bf74-e1b00e2a3daa"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"environment":{"environmentId":"8a97ba97-e9b1-4f61-84c3-da4df3faf8d4","workspaceId":"1dc9d7e2-e8b4-44ea-abc5-3b0e24119da9"},"lakehouse":{"known_lakehouses":[{"id":"c376541a-203d-4803-8f9a-a569b1645c00"}],"default_lakehouse":"c376541a-203d-4803-8f9a-a569b1645c00","default_lakehouse_name":"stock_project_lakehouse","default_lakehouse_workspace_id":"1dc9d7e2-e8b4-44ea-abc5-3b0e24119da9"}}},"nbformat":4,"nbformat_minor":5}